            <section>
                <h2>强化学习：GRPO 优化算法</h2>
                <div class="insight-box">
                    <strong>核心观点：</strong>采用 GRPO 算法，通过比较一组轨迹的相对好坏进行优化，比 PPO 更适合奖励复杂场景
                </div>

                <h3>为什么不用传统 PPO？</h3>
                <ul>
                    <li>PPO 依赖学习出的<strong>价值函数 (Value Function)</strong></li>
                    <li>当奖励函数复杂多变时，价值函数很难学准</li>
                    <li>导致训练不稳定</li>
                </ul>

                <h3>GRPO 的思想</h3>
                <div class="solution-box">
                    <p><strong>Group Relative Policy Optimization</strong> (组相对策略优化)</p>
                    <p><strong>核心：</strong>一次生成 k 条轨迹，在组内进行"横向比较"，不依赖不稳定的价值函数，而是用<span class="highlight">相对优势</span></p>
                </div>

                <div class="two-columns">
                    <div>
                        <h3>优势估计 (Advantage Estimation)</h3>
                        <div class="formula-box">
                            \[\hat{A}_i = \frac{R_i - \text{mean}(\{R_j\})}{\text{std}(\{R_j\}) + \epsilon}\]
                        </div>
                        <ul>
                            <li>\(R_i\): 轨迹 i 的奖励</li>
                            <li>相对于同组其他轨迹的标准化优势</li>
                        </ul>

                        <h3>GRPO 损失函数</h3>
                        <div class="formula-box" style="font-size: 0.95em;">
                            \[\mathcal{L}_{\text{GRPO}}(\theta) = -\mathbb{E} \left[ \sum_i \sum_t \min \left( r_{i,t}(\theta) \hat{A}_i, \text{clip}(r_{i,t}(\theta), 1-\varepsilon, 1+\varepsilon) \hat{A}_i \right) \right]\]
                        </div>
                    </div>

                    <div>
                        <h3>GRPO 的优势</h3>
                        <ol>
                            <li><strong>相对信号更稳定</strong>
                                <ul>
                                    <li>不需要绝对价值估计</li>
                                    <li>只需知道"轨迹 i 比同组其他轨迹好多少"</li>
                                </ul>
                            </li>
                            <li><strong>适合复杂奖励</strong>
                                <ul>
                                    <li>面对混合奖励系统依然稳健</li>
                                    <li>不受单个奖励函数波动的影响</li>
                                </ul>
                            </li>
                            <li><strong>训练效率高</strong>
                                <ul>
                                    <li>无需训练单独的价值网络</li>
                                    <li>减少计算开销</li>
                                </ul>
                            </li>
                        </ol>

                        <div class="insight-box" style="margin-top: 20px;">
                            <strong>工作流程：</strong>采样 k 条轨迹 → 计算奖励 → 组内标准化得到相对优势 → 更新策略 → 重复
                        </div>
                    </div>
                </div>
            </section>
